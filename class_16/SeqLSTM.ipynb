{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import spacy\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f = open('../../data/sentiment/positive')\n",
    "# pos = f.read()\n",
    "# f.close()\n",
    "\n",
    "# f = open('../../data/sentiment/negative')\n",
    "# neg = f.read()\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Spacy word embeddings\n",
    "word_embeddings = spacy.load('en', vectors='glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create a function to get vector format data for a sequence\n",
    "def sequence_to_data(seq, max_len=None):\n",
    "    seq = unicode(seq)\n",
    "    data = [word_embeddings(ix).vector for ix in seq.split()]\n",
    "    \n",
    "    if max_len is None:\n",
    "        max_len = len(data)\n",
    "    \n",
    "    data_mat = np.zeros((1, max_len, 300))\n",
    "    \n",
    "    for ix in range(min(len(data), max_len)):\n",
    "        data_mat[:, ix, :] = data[ix]\n",
    "    \n",
    "    return data_mat\n",
    "\n",
    "def seq_data_matrix(seq_data, max_len=None):\n",
    "    data = np.concatenate([sequence_to_data(ix, max_len) for ix in seq_data], axis=0)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "# q = sequence_to_data(u'hello! what is the date today?', 20)\n",
    "# print q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3, 10, 300)\n"
     ]
    }
   ],
   "source": [
    "s = [u'hello! what is the date today?', u'This is my new sequence', u'just a randome sequence']\n",
    "all_data = []\n",
    "for ix in s:\n",
    "    v = sequence_to_data(ix, 10)\n",
    "    all_data.append(v)\n",
    "print len(all_data)\n",
    "\n",
    "d = np.concatenate(all_data, axis=0)\n",
    "print d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a very nice sequence\n",
      "[u'This', u'is', u'a', u'very', u'nice', u'sequence']\n",
      "(1, 5, 300) 0.0\n",
      "[[ 1.57125245e-02 -1.48509129e-02  9.06997892e-05 -1.63511020e-02\n",
      "  -8.86286839e-03]]\n"
     ]
    }
   ],
   "source": [
    "# s = u'This is a very nice sequence'\n",
    "# print s\n",
    "\n",
    "# words = s.split()\n",
    "# print words\n",
    "\n",
    "# vecs = []\n",
    "\n",
    "# for ix in words:\n",
    "#     vecs.append(word_embeddings(ix).vector)\n",
    "# # print vecs\n",
    "\n",
    "# data = np.zeros((1, 5, 300))\n",
    "# print data.shape, data.mean()\n",
    "\n",
    "# m = min(len(words), 5)\n",
    "\n",
    "# for ix in range(m):\n",
    "#     data[:, ix, :] = vecs[ix]\n",
    "\n",
    "# print data.mean(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame([], columns=['text', 'score'])\n",
    "# for ix in pos.split('\\n'):\n",
    "#     text = ix.strip().lower()\n",
    "#     if len(text) > 1:\n",
    "#         df = df.append({'text': text, 'score': 1}, ignore_index=True)\n",
    "#     # print sequence_to_data(ix.strip().lower()).shape\n",
    "\n",
    "# for ix in neg.split('\\n'):\n",
    "#     text = ix.strip().lower()\n",
    "#     if len(text) > 1:\n",
    "#         df = df.append({'text': text, 'score': 0}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = sklearn.utils.shuffle(df).reset_index(drop=True)\n",
    "df = pd.read_csv('../../data/sentiment/dataset.csv', sep='|', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a gushy episode of  m  a  s  h  only this time...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the storys pathetic and the gags are puerile</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not only a comingofage story and cautionary pa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beyond a handful of mildly amusing lines    th...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a complex psychological drama about a father w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score\n",
       "0  a gushy episode of  m  a  s  h  only this time...    0.0\n",
       "1       the storys pathetic and the gags are puerile    0.0\n",
       "2  not only a comingofage story and cautionary pa...    1.0\n",
       "3  beyond a handful of mildly amusing lines    th...    0.0\n",
       "4  a complex psychological drama about a father w...    1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.text.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['len'] = df['text'].str.split().apply(lambda x: len(x))\n",
    "# df = df.sort_index(ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a gushy episode of  m  a  s  h  only this time...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the storys pathetic and the gags are puerile</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not only a comingofage story and cautionary pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beyond a handful of mildly amusing lines    th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a complex psychological drama about a father w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  len\n",
       "0  a gushy episode of  m  a  s  h  only this time...    0.0   15\n",
       "1       the storys pathetic and the gags are puerile    0.0    8\n",
       "2  not only a comingofage story and cautionary pa...    1.0   15\n",
       "3  beyond a handful of mildly amusing lines    th...    0.0   14\n",
       "4  a complex psychological drama about a father w...    1.0   16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.to_csv('../../data/sentiment/dataset.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bucket_sizes = [[0, 10], [10, 15], [15, 20], [20, 25], [25, 45]]\n",
    "\n",
    "def assign_bucket(x):\n",
    "    for bucket in bucket_sizes:\n",
    "        if x > bucket[0] and x <= bucket[1]:\n",
    "            return bucket_sizes.index(bucket)\n",
    "    return len(bucket_sizes)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a gushy episode of  m  a  s  h  only this time...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the storys pathetic and the gags are puerile</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not only a comingofage story and cautionary pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beyond a handful of mildly amusing lines    th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a complex psychological drama about a father w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  len  bucket\n",
       "0  a gushy episode of  m  a  s  h  only this time...    0.0   15       1\n",
       "1       the storys pathetic and the gags are puerile    0.0    8       0\n",
       "2  not only a comingofage story and cautionary pa...    1.0   15       1\n",
       "3  beyond a handful of mildly amusing lines    th...    0.0   14       1\n",
       "4  a complex psychological drama about a father w...    1.0   16       2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bucket'] = df.len.apply(assign_bucket)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9136</th>\n",
       "      <td>a brisk  reverent  and subtly different sequel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>too silly to take seriously</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6700</th>\n",
       "      <td>the date movie that franz kafka would have made</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3929</th>\n",
       "      <td>an inexperienced director  mehta has much to l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8836</th>\n",
       "      <td>an uneven mix of dark satire and childhood awa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  score  len  bucket\n",
       "9136     a brisk  reverent  and subtly different sequel    1.0    7       0\n",
       "1777                        too silly to take seriously    0.0    5       0\n",
       "6700    the date movie that franz kafka would have made    1.0    9       0\n",
       "3929  an inexperienced director  mehta has much to l...    0.0    8       0\n",
       "8836  an uneven mix of dark satire and childhood awa...    0.0    9       0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort(columns=['bucket'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>more busy than exciting  more frantic than inv...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5593</th>\n",
       "      <td>ludicrous  but director carl franklin adds eno...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>donovan    squanders his main asset  jackie ch...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7115</th>\n",
       "      <td>you could nap for an hour and not miss a thing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>my little eye is the best little  horror  movi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>an uglyduckling tale so hideously and clumsily...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7189</th>\n",
       "      <td>light  silly  photographed with colour and dep...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7151</th>\n",
       "      <td>admirable  certainly  but not much fun to watc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7193</th>\n",
       "      <td>what might have been a predictably heartwarmin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>a potentially good comic premise and excellent...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5576</th>\n",
       "      <td>the film sparkles with the the wisdom and humo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>an otherwise intense  twistandturn thriller th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7204</th>\n",
       "      <td>occasionally interesting but essentially unper...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7165</th>\n",
       "      <td>much smarter and more attentive than it first ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7111</th>\n",
       "      <td>i hope the movie is widely seen and debated wi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>the latest vapid actors exercise to appropriat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7114</th>\n",
       "      <td>its a movie  and an album  you wont want to miss</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>enjoy it for what it is  you can hate yourself...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5585</th>\n",
       "      <td>the mood  look and tone of the film fit the in...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>a movie in which laughter and selfexploitation...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>all prints of this film should be sent to and ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7154</th>\n",
       "      <td>the christ allegory doesnt work because there ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>an immensely entertaining look at some of the ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>a hypnotic cyber hymn and a cruel story of you...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>this film seems thirsty for reflection  itself...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5583</th>\n",
       "      <td>the truth is that the truth about charlie gets...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>subversive  meditative  clinical and poetic  t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207</th>\n",
       "      <td>safe conduct  however ambitious and wellintent...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>a feelgood movie that doesnt give you enough t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7228</th>\n",
       "      <td>its sincere to a fault  but  unfortunately  no...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8452</th>\n",
       "      <td>it is just too bad the films story does not li...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8385</th>\n",
       "      <td>the work of an exhausted  desiccated talent wh...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>a swashbuckling tale of love  betrayal  reveng...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8523</th>\n",
       "      <td>a movie i loved on first sight and  even more ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>perhaps even the slc high command found writer...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7636</th>\n",
       "      <td>thirty years ago  it would have been groundbre...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>a solidly constructed  entertaining thriller t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>theres an audience for it  but it could have b...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>sodden and glum  even in those moments where i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>a very funny look at how another culture handl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>the films tone and pacing are off almost from ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950</th>\n",
       "      <td>infidelity drama is nicely shot  welledited an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>the words  frankly  my dear  i dont give a dam...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>often silly  and gross  but its rarely as moro...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7732</th>\n",
       "      <td>a very good film sits in the place where a mas...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>exposing the ways we fool ourselves is one hou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8447</th>\n",
       "      <td>the actors are appealing  but elysian fields i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>my god  im behaving like an idiot   yes  you a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>topics that could make a sailor blush  but lot...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>an incoherent jumble of a film thats rarely as...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8488</th>\n",
       "      <td>charly comes off as emotionally manipulative a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7677</th>\n",
       "      <td>vividly conveys both the pitfalls and the plea...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8474</th>\n",
       "      <td>if the reallife story is genuinely inspiration...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>shamelessly sappy and  worse  runs away from i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8475</th>\n",
       "      <td>a sad  superior human comedy played out on the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>the predominantly amateur cast is painful to w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8580</th>\n",
       "      <td>pretend like your sat scores are below 120 and...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7717</th>\n",
       "      <td>in imax in short  its just as wonderful on the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>this extremely unfunny film clocks in at 80 mi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7715</th>\n",
       "      <td>its surprisingly decent  particularly for a te...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2081 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  score  len  bucket\n",
       "3424  more busy than exciting  more frantic than inv...    0.0   12       1\n",
       "5593  ludicrous  but director carl franklin adds eno...    1.0   14       1\n",
       "5027  donovan    squanders his main asset  jackie ch...    0.0   13       1\n",
       "7115     you could nap for an hour and not miss a thing    0.0   11       1\n",
       "5004  my little eye is the best little  horror  movi...    1.0   13       1\n",
       "3370  an uglyduckling tale so hideously and clumsily...    0.0   11       1\n",
       "7189  light  silly  photographed with colour and dep...    1.0   12       1\n",
       "7151  admirable  certainly  but not much fun to watc...    0.0   12       1\n",
       "7193  what might have been a predictably heartwarmin...    1.0   12       1\n",
       "3425  a potentially good comic premise and excellent...    0.0   11       1\n",
       "5576  the film sparkles with the the wisdom and humo...    1.0   12       1\n",
       "5550  an otherwise intense  twistandturn thriller th...    1.0   13       1\n",
       "7204  occasionally interesting but essentially unper...    0.0   12       1\n",
       "7165  much smarter and more attentive than it first ...    1.0   12       1\n",
       "7111  i hope the movie is widely seen and debated wi...    1.0   14       1\n",
       "3452  the latest vapid actors exercise to appropriat...    0.0   13       1\n",
       "7114   its a movie  and an album  you wont want to miss    1.0   11       1\n",
       "5036  enjoy it for what it is  you can hate yourself...    1.0   11       1\n",
       "5585  the mood  look and tone of the film fit the in...    1.0   15       1\n",
       "3372  a movie in which laughter and selfexploitation...    0.0   12       1\n",
       "3360  all prints of this film should be sent to and ...    0.0   13       1\n",
       "7154  the christ allegory doesnt work because there ...    0.0   12       1\n",
       "3422  an immensely entertaining look at some of the ...    1.0   15       1\n",
       "3341  a hypnotic cyber hymn and a cruel story of you...    1.0   11       1\n",
       "3343  this film seems thirsty for reflection  itself...    1.0   11       1\n",
       "5583  the truth is that the truth about charlie gets...    0.0   11       1\n",
       "5025  subversive  meditative  clinical and poetic  t...    1.0   14       1\n",
       "7207  safe conduct  however ambitious and wellintent...    0.0   12       1\n",
       "4985  a feelgood movie that doesnt give you enough t...    0.0   12       1\n",
       "7228  its sincere to a fault  but  unfortunately  no...    0.0   13       1\n",
       "...                                                 ...    ...  ...     ...\n",
       "8452  it is just too bad the films story does not li...    0.0   15       1\n",
       "8385  the work of an exhausted  desiccated talent wh...    0.0   15       1\n",
       "2955  a swashbuckling tale of love  betrayal  reveng...    1.0   11       1\n",
       "8523  a movie i loved on first sight and  even more ...    1.0   14       1\n",
       "8585  perhaps even the slc high command found writer...    0.0   15       1\n",
       "7636  thirty years ago  it would have been groundbre...    0.0   12       1\n",
       "2879  a solidly constructed  entertaining thriller t...    1.0   11       1\n",
       "2834  theres an audience for it  but it could have b...    0.0   14       1\n",
       "2128  sodden and glum  even in those moments where i...    0.0   15       1\n",
       "2891  a very funny look at how another culture handl...    1.0   15       1\n",
       "2949  the films tone and pacing are off almost from ...    0.0   11       1\n",
       "2950  infidelity drama is nicely shot  welledited an...    1.0   14       1\n",
       "8533  the words  frankly  my dear  i dont give a dam...    0.0   15       1\n",
       "2951  often silly  and gross  but its rarely as moro...    0.0   14       1\n",
       "7732  a very good film sits in the place where a mas...    1.0   13       1\n",
       "2157  exposing the ways we fool ourselves is one hou...    1.0   12       1\n",
       "8447  the actors are appealing  but elysian fields i...    0.0   12       1\n",
       "2080  my god  im behaving like an idiot   yes  you a...    0.0   12       1\n",
       "2082  topics that could make a sailor blush  but lot...    1.0   11       1\n",
       "2904  an incoherent jumble of a film thats rarely as...    0.0   15       1\n",
       "8488  charly comes off as emotionally manipulative a...    0.0   15       1\n",
       "7677  vividly conveys both the pitfalls and the plea...    1.0   11       1\n",
       "8474  if the reallife story is genuinely inspiration...    1.0   13       1\n",
       "2110  shamelessly sappy and  worse  runs away from i...    0.0   11       1\n",
       "8475  a sad  superior human comedy played out on the...    1.0   13       1\n",
       "2823  the predominantly amateur cast is painful to w...    0.0   15       1\n",
       "8580  pretend like your sat scores are below 120 and...    0.0   15       1\n",
       "7717  in imax in short  its just as wonderful on the...    1.0   12       1\n",
       "2081  this extremely unfunny film clocks in at 80 mi...    0.0   14       1\n",
       "7715  its surprisingly decent  particularly for a te...    1.0   11       1\n",
       "\n",
       "[2081 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.bucket == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_batch(data, batch_size=10, gpu=True):\n",
    "    for bx in range(len(bucket_sizes)):\n",
    "        bucket_data = df[(df.bucket == bx)].reset_index(drop=True)\n",
    "        # print bx, bucket_sizes[bx][1], bucket_data.shape\n",
    "        \n",
    "        start = 0\n",
    "        stop = start + batch_size\n",
    "        \n",
    "        while start < bucket_data.shape[0]:\n",
    "            seq_length = bucket_sizes[bx][1]\n",
    "            section = bucket_data[start:stop]\n",
    "            X_data = seq_data_matrix(section.text, max_len=seq_length)\n",
    "            y_data = section.score\n",
    "            \n",
    "            if gpu:\n",
    "                yield Variable(torch.FloatTensor(X_data).cuda(), requires_grad=True), Variable(torch.LongTensor(y_data)).cuda()\n",
    "            else:\n",
    "                yield Variable(torch.FloatTensor(X_data), requires_grad=True), Variable(torch.LongTensor(y_data))\n",
    "            \n",
    "            start = stop\n",
    "            stop = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10, 300]) torch.Size([1000])\n",
      "torch.Size([1000, 10, 300]) torch.Size([1000])\n",
      "torch.Size([49, 10, 300]) torch.Size([49])\n",
      "torch.Size([1000, 15, 300]) torch.Size([1000])\n",
      "torch.Size([1000, 15, 300]) torch.Size([1000])\n",
      "torch.Size([81, 15, 300]) torch.Size([81])\n",
      "torch.Size([1000, 20, 300]) torch.Size([1000])\n",
      "torch.Size([1000, 20, 300]) torch.Size([1000])\n",
      "torch.Size([333, 20, 300]) torch.Size([333])\n",
      "torch.Size([1000, 25, 300]) torch.Size([1000])\n",
      "torch.Size([968, 25, 300]) torch.Size([968])\n",
      "torch.Size([1000, 45, 300]) torch.Size([1000])\n",
      "torch.Size([1000, 45, 300]) torch.Size([1000])\n",
      "torch.Size([231, 45, 300]) torch.Size([231])\n"
     ]
    }
   ],
   "source": [
    "for ix, iy in make_batch(df, batch_size=1000, gpu=False):\n",
    "    print ix.shape, iy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.head(10)\n",
    "# Printing colored text (Useful later)\n",
    "# print colored(\"hello red world\", 'blue')# print 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SeqModel(nn.Module):\n",
    "    def __init__(self, in_shape=None, out_shape=None, hidden_shape=None):\n",
    "        super(SeqModel, self).__init__()\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "        self.hidden_shape = hidden_shape\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.in_shape,\n",
    "            hidden_size=self.hidden_shape,\n",
    "            num_layers=self.n_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lin = nn.Linear(self.hidden_shape, 64)\n",
    "        self.dropout = nn.Dropout(0.42)\n",
    "        self.out = nn.Linear(64, self.out_shape)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        r_out, h_state = self.rnn(x, h)\n",
    "        last_out = r_out[:, -1, :]\n",
    "        y = F.tanh(self.lin(last_out))\n",
    "        y = self.dropout(y)\n",
    "        y = F.softmax(self.out(y))\n",
    "        return y\n",
    "    \n",
    "    def predict(self, x):\n",
    "        h_state = self.init_hidden(1, gpu=False)\n",
    "        \n",
    "        x = sequence_to_data(x)\n",
    "        pred = self.forward(torch.FloatTensor(x), h_state)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        h_state = self.init_hidden(1, gpu=False)\n",
    "        \n",
    "        x = sequence_to_data(x)\n",
    "        r_out, h = self.rnn(torch.FloatTensor(x), h_state)\n",
    "        last_out = r_out[:, -1, :]\n",
    "        \n",
    "        return last_out.data.numpy()\n",
    "            \n",
    "    def init_hidden(self, batch_size, gpu=True):\n",
    "        if gpu:\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape).cuda()),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)).cuda())\n",
    "        return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)),\n",
    "                Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqModel(\n",
      "  (rnn): LSTM(300, 25, batch_first=True)\n",
      "  (lin): Linear(in_features=25, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.42)\n",
      "  (out): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SeqModel(\n",
       "  (rnn): LSTM(300, 25, batch_first=True)\n",
       "  (lin): Linear(in_features=25, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.42)\n",
       "  (out): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SeqModel(in_shape=300, hidden_shape=25, out_shape=2)\n",
    "\n",
    "print model\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.predict('hello bad world')\n",
    "\n",
    "# Load the model\n",
    "# model.load_state_dict(torch.load('/home/shubham/all_projects/CB/Summer_2018/data/checkpoints/seq_lstm/model_256h_epoch_240.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.69742333889 at Epoch: 0 | Step: 0\n",
      "Loss: 0.69163531065 at Epoch: 0 | Step: 20\n",
      "Loss: 0.692053377628 at Epoch: 0 | Step: 40\n",
      "Overall Average Loss: 0.694422006607 at Epoch: 0\n",
      "Loss: 0.692090451717 at Epoch: 1 | Step: 0\n",
      "Loss: 0.687447428703 at Epoch: 1 | Step: 20\n",
      "Loss: 0.689199209213 at Epoch: 1 | Step: 40\n",
      "Overall Average Loss: 0.689712703228 at Epoch: 1\n",
      "Loss: 0.681926727295 at Epoch: 2 | Step: 0\n",
      "Loss: 0.668727934361 at Epoch: 2 | Step: 20\n",
      "Loss: 0.647350132465 at Epoch: 2 | Step: 40\n",
      "Overall Average Loss: 0.674889683723 at Epoch: 2\n",
      "Loss: 0.63601154089 at Epoch: 3 | Step: 0\n",
      "Loss: 0.608234405518 at Epoch: 3 | Step: 20\n",
      "Loss: 0.593071639538 at Epoch: 3 | Step: 40\n",
      "Overall Average Loss: 0.638657689095 at Epoch: 3\n",
      "Loss: 0.577009916306 at Epoch: 4 | Step: 0\n",
      "Loss: 0.578892111778 at Epoch: 4 | Step: 20\n",
      "Loss: 0.581489622593 at Epoch: 4 | Step: 40\n",
      "Overall Average Loss: 0.612688839436 at Epoch: 4\n",
      "Loss: 0.549013614655 at Epoch: 5 | Step: 0\n",
      "Loss: 0.570038259029 at Epoch: 5 | Step: 20\n",
      "Loss: 0.575555443764 at Epoch: 5 | Step: 40\n",
      "Overall Average Loss: 0.593836128712 at Epoch: 5\n",
      "Loss: 0.526914358139 at Epoch: 6 | Step: 0\n",
      "Loss: 0.56396317482 at Epoch: 6 | Step: 20\n",
      "Loss: 0.570190310478 at Epoch: 6 | Step: 40\n",
      "Overall Average Loss: 0.584912419319 at Epoch: 6\n",
      "Loss: 0.51433557272 at Epoch: 7 | Step: 0\n",
      "Loss: 0.557445347309 at Epoch: 7 | Step: 20\n",
      "Loss: 0.571662664413 at Epoch: 7 | Step: 40\n",
      "Overall Average Loss: 0.582174420357 at Epoch: 7\n",
      "Loss: 0.525627613068 at Epoch: 8 | Step: 0\n",
      "Loss: 0.543232917786 at Epoch: 8 | Step: 20\n",
      "Loss: 0.556832849979 at Epoch: 8 | Step: 40\n",
      "Overall Average Loss: 0.57739084959 at Epoch: 8\n",
      "Loss: 0.525994777679 at Epoch: 9 | Step: 0\n",
      "Loss: 0.545738399029 at Epoch: 9 | Step: 20\n",
      "Loss: 0.544856071472 at Epoch: 9 | Step: 40\n",
      "Overall Average Loss: 0.570654571056 at Epoch: 9\n",
      "Loss: 0.513692259789 at Epoch: 10 | Step: 0\n",
      "Loss: 0.541399240494 at Epoch: 10 | Step: 20\n",
      "Loss: 0.542103528976 at Epoch: 10 | Step: 40\n",
      "Overall Average Loss: 0.563399195671 at Epoch: 10\n",
      "Loss: 0.505694925785 at Epoch: 11 | Step: 0\n",
      "Loss: 0.533120155334 at Epoch: 11 | Step: 20\n",
      "Loss: 0.536459207535 at Epoch: 11 | Step: 40\n",
      "Overall Average Loss: 0.557379901409 at Epoch: 11\n",
      "Loss: 0.493396878242 at Epoch: 12 | Step: 0\n",
      "Loss: 0.526925086975 at Epoch: 12 | Step: 20\n",
      "Loss: 0.531074225903 at Epoch: 12 | Step: 40\n",
      "Overall Average Loss: 0.551092982292 at Epoch: 12\n",
      "Loss: 0.489806681871 at Epoch: 13 | Step: 0\n",
      "Loss: 0.519273877144 at Epoch: 13 | Step: 20\n",
      "Loss: 0.519293963909 at Epoch: 13 | Step: 40\n",
      "Overall Average Loss: 0.545694589615 at Epoch: 13\n",
      "Loss: 0.48345375061 at Epoch: 14 | Step: 0\n",
      "Loss: 0.51608812809 at Epoch: 14 | Step: 20\n",
      "Loss: 0.513243317604 at Epoch: 14 | Step: 40\n",
      "Overall Average Loss: 0.541397333145 at Epoch: 14\n",
      "Loss: 0.481462657452 at Epoch: 15 | Step: 0\n",
      "Loss: 0.510341882706 at Epoch: 15 | Step: 20\n",
      "Loss: 0.511853575706 at Epoch: 15 | Step: 40\n",
      "Overall Average Loss: 0.537546873093 at Epoch: 15\n",
      "Loss: 0.480911433697 at Epoch: 16 | Step: 0\n",
      "Loss: 0.513807117939 at Epoch: 16 | Step: 20\n",
      "Loss: 0.509938836098 at Epoch: 16 | Step: 40\n",
      "Overall Average Loss: 0.534784376621 at Epoch: 16\n",
      "Loss: 0.47408130765 at Epoch: 17 | Step: 0\n",
      "Loss: 0.50886631012 at Epoch: 17 | Step: 20\n",
      "Loss: 0.510581076145 at Epoch: 17 | Step: 40\n",
      "Overall Average Loss: 0.530485510826 at Epoch: 17\n",
      "Loss: 0.470617681742 at Epoch: 18 | Step: 0\n",
      "Loss: 0.505066335201 at Epoch: 18 | Step: 20\n",
      "Loss: 0.51499325037 at Epoch: 18 | Step: 40\n",
      "Overall Average Loss: 0.52730178833 at Epoch: 18\n",
      "Loss: 0.46736240387 at Epoch: 19 | Step: 0\n",
      "Loss: 0.505021572113 at Epoch: 19 | Step: 20\n",
      "Loss: 0.512701034546 at Epoch: 19 | Step: 40\n",
      "Overall Average Loss: 0.524192869663 at Epoch: 19\n",
      "Loss: 0.470387965441 at Epoch: 20 | Step: 0\n",
      "Loss: 0.504788458347 at Epoch: 20 | Step: 20\n",
      "Loss: 0.508899629116 at Epoch: 20 | Step: 40\n",
      "Overall Average Loss: 0.523307919502 at Epoch: 20\n",
      "Loss: 0.464465528727 at Epoch: 21 | Step: 0\n",
      "Loss: 0.507555365562 at Epoch: 21 | Step: 20\n",
      "Loss: 0.515251517296 at Epoch: 21 | Step: 40\n",
      "Overall Average Loss: 0.52463054657 at Epoch: 21\n",
      "Loss: 0.469170838594 at Epoch: 22 | Step: 0\n",
      "Loss: 0.501567602158 at Epoch: 22 | Step: 20\n",
      "Loss: 0.50456982851 at Epoch: 22 | Step: 40\n",
      "Overall Average Loss: 0.526973605156 at Epoch: 22\n",
      "Loss: 0.485012233257 at Epoch: 23 | Step: 0\n",
      "Loss: 0.498085379601 at Epoch: 23 | Step: 20\n",
      "Loss: 0.495019763708 at Epoch: 23 | Step: 40\n",
      "Overall Average Loss: 0.523140490055 at Epoch: 23\n",
      "Loss: 0.466693073511 at Epoch: 24 | Step: 0\n",
      "Loss: 0.485822230577 at Epoch: 24 | Step: 20\n",
      "Loss: 0.494245409966 at Epoch: 24 | Step: 40\n",
      "Overall Average Loss: 0.514452755451 at Epoch: 24\n",
      "Loss: 0.471115678549 at Epoch: 25 | Step: 0\n",
      "Loss: 0.483766198158 at Epoch: 25 | Step: 20\n",
      "Loss: 0.489568740129 at Epoch: 25 | Step: 40\n",
      "Overall Average Loss: 0.512136101723 at Epoch: 25\n",
      "Loss: 0.474546521902 at Epoch: 26 | Step: 0\n",
      "Loss: 0.48164254427 at Epoch: 26 | Step: 20\n",
      "Loss: 0.486355632544 at Epoch: 26 | Step: 40\n",
      "Overall Average Loss: 0.511702656746 at Epoch: 26\n",
      "Loss: 0.477392196655 at Epoch: 27 | Step: 0\n",
      "Loss: 0.495191037655 at Epoch: 27 | Step: 20\n",
      "Loss: 0.496343642473 at Epoch: 27 | Step: 40\n",
      "Overall Average Loss: 0.518317103386 at Epoch: 27\n",
      "Loss: 0.456563651562 at Epoch: 28 | Step: 0\n",
      "Loss: 0.484345287085 at Epoch: 28 | Step: 20\n",
      "Loss: 0.490119546652 at Epoch: 28 | Step: 40\n",
      "Overall Average Loss: 0.513072371483 at Epoch: 28\n",
      "Loss: 0.451302081347 at Epoch: 29 | Step: 0\n",
      "Loss: 0.479845076799 at Epoch: 29 | Step: 20\n",
      "Loss: 0.487130314112 at Epoch: 29 | Step: 40\n",
      "Overall Average Loss: 0.506174564362 at Epoch: 29\n",
      "Loss: 0.451211124659 at Epoch: 30 | Step: 0\n",
      "Loss: 0.475132018328 at Epoch: 30 | Step: 20\n",
      "Loss: 0.484574496746 at Epoch: 30 | Step: 40\n",
      "Overall Average Loss: 0.502227902412 at Epoch: 30\n",
      "Loss: 0.451631844044 at Epoch: 31 | Step: 0\n",
      "Loss: 0.475101321936 at Epoch: 31 | Step: 20\n",
      "Loss: 0.487421482801 at Epoch: 31 | Step: 40\n",
      "Overall Average Loss: 0.498895674944 at Epoch: 31\n",
      "Loss: 0.447932511568 at Epoch: 32 | Step: 0\n",
      "Loss: 0.474938631058 at Epoch: 32 | Step: 20\n",
      "Loss: 0.484663814306 at Epoch: 32 | Step: 40\n",
      "Overall Average Loss: 0.497542649508 at Epoch: 32\n",
      "Loss: 0.444742619991 at Epoch: 33 | Step: 0\n",
      "Loss: 0.471693068743 at Epoch: 33 | Step: 20\n",
      "Loss: 0.484110414982 at Epoch: 33 | Step: 40\n",
      "Overall Average Loss: 0.493412494659 at Epoch: 33\n",
      "Loss: 0.442980676889 at Epoch: 34 | Step: 0\n",
      "Loss: 0.467999815941 at Epoch: 34 | Step: 20\n",
      "Loss: 0.478754997253 at Epoch: 34 | Step: 40\n",
      "Overall Average Loss: 0.492076158524 at Epoch: 34\n",
      "Loss: 0.440233498812 at Epoch: 35 | Step: 0\n",
      "Loss: 0.470312267542 at Epoch: 35 | Step: 20\n",
      "Loss: 0.476249724627 at Epoch: 35 | Step: 40\n",
      "Overall Average Loss: 0.489179372787 at Epoch: 35\n",
      "Loss: 0.439033806324 at Epoch: 36 | Step: 0\n",
      "Loss: 0.469206631184 at Epoch: 36 | Step: 20\n",
      "Loss: 0.475227326155 at Epoch: 36 | Step: 40\n",
      "Overall Average Loss: 0.49189427495 at Epoch: 36\n",
      "Loss: 0.436659693718 at Epoch: 37 | Step: 0\n",
      "Loss: 0.491647422314 at Epoch: 37 | Step: 20\n",
      "Loss: 0.472197830677 at Epoch: 37 | Step: 40\n",
      "Overall Average Loss: 0.505670905113 at Epoch: 37\n",
      "Loss: 0.457237482071 at Epoch: 38 | Step: 0\n",
      "Loss: 0.466650813818 at Epoch: 38 | Step: 20\n",
      "Loss: 0.481216132641 at Epoch: 38 | Step: 40\n",
      "Overall Average Loss: 0.500137031078 at Epoch: 38\n",
      "Loss: 0.47481316328 at Epoch: 39 | Step: 0\n",
      "Loss: 0.467495381832 at Epoch: 39 | Step: 20\n",
      "Loss: 0.470978289843 at Epoch: 39 | Step: 40\n",
      "Overall Average Loss: 0.496206909418 at Epoch: 39\n",
      "Loss: 0.461064308882 at Epoch: 40 | Step: 0\n",
      "Loss: 0.464761227369 at Epoch: 40 | Step: 20\n",
      "Loss: 0.46520626545 at Epoch: 40 | Step: 40\n",
      "Overall Average Loss: 0.491576761007 at Epoch: 40\n",
      "Loss: 0.469571202993 at Epoch: 41 | Step: 0\n",
      "Loss: 0.459188580513 at Epoch: 41 | Step: 20\n",
      "Loss: 0.462537109852 at Epoch: 41 | Step: 40\n",
      "Overall Average Loss: 0.486786901951 at Epoch: 41\n",
      "Loss: 0.469054400921 at Epoch: 42 | Step: 0\n",
      "Loss: 0.459449231625 at Epoch: 42 | Step: 20\n",
      "Loss: 0.463728100061 at Epoch: 42 | Step: 40\n",
      "Overall Average Loss: 0.484697341919 at Epoch: 42\n",
      "Loss: 0.457308411598 at Epoch: 43 | Step: 0\n",
      "Loss: 0.455283880234 at Epoch: 43 | Step: 20\n",
      "Loss: 0.461940914392 at Epoch: 43 | Step: 40\n",
      "Overall Average Loss: 0.485280305147 at Epoch: 43\n",
      "Loss: 0.457079768181 at Epoch: 44 | Step: 0\n",
      "Loss: 0.451515763998 at Epoch: 44 | Step: 20\n",
      "Loss: 0.457645446062 at Epoch: 44 | Step: 40\n",
      "Overall Average Loss: 0.482401043177 at Epoch: 44\n",
      "Loss: 0.455950021744 at Epoch: 45 | Step: 0\n",
      "Loss: 0.45035892725 at Epoch: 45 | Step: 20\n",
      "Loss: 0.46109944582 at Epoch: 45 | Step: 40\n",
      "Overall Average Loss: 0.482721477747 at Epoch: 45\n",
      "Loss: 0.450999975204 at Epoch: 46 | Step: 0\n",
      "Loss: 0.457892537117 at Epoch: 46 | Step: 20\n",
      "Loss: 0.473802834749 at Epoch: 46 | Step: 40\n",
      "Overall Average Loss: 0.484114319086 at Epoch: 46\n",
      "Loss: 0.439380735159 at Epoch: 47 | Step: 0\n",
      "Loss: 0.456071287394 at Epoch: 47 | Step: 20\n",
      "Loss: 0.470109641552 at Epoch: 47 | Step: 40\n",
      "Overall Average Loss: 0.479265272617 at Epoch: 47\n",
      "Loss: 0.43844383955 at Epoch: 48 | Step: 0\n",
      "Loss: 0.446683079004 at Epoch: 48 | Step: 20\n",
      "Loss: 0.470870375633 at Epoch: 48 | Step: 40\n",
      "Overall Average Loss: 0.478016227484 at Epoch: 48\n",
      "Loss: 0.448094546795 at Epoch: 49 | Step: 0\n",
      "Loss: 0.448546677828 at Epoch: 49 | Step: 20\n",
      "Loss: 0.494729071856 at Epoch: 49 | Step: 40\n",
      "Overall Average Loss: 0.481781721115 at Epoch: 49\n"
     ]
    }
   ],
   "source": [
    "# Set to train mode\n",
    "# model.cuda()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    N = 0\n",
    "    for step, (b_x, b_y) in enumerate(make_batch(df, batch_size=200)):\n",
    "        # print step, b_x.shape, b_y.shape\n",
    "        bsize = b_x.size(0)\n",
    "        lol = b_x\n",
    "        h_state = model.init_hidden(bsize, gpu=True)\n",
    "\n",
    "        pred = model(b_x, h_state)\n",
    "        loss = criterion(pred, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss\n",
    "        N += 1.0\n",
    "        if step%20 == 0:\n",
    "            print 'Loss: {} at Epoch: {} | Step: {}'.format(loss, epoch, step)\n",
    "        \n",
    "    print \"Overall Average Loss: {} at Epoch: {}\".format(total_loss / float(N), epoch)\n",
    "    \n",
    "    # Save model checkpoints\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), \"/home/shubham/all_projects/CB/Summer_2018/data/checkpoints/seq_lstm_bucket/model_256h_epoch_{}.ckpt\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25) (1, 25)\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "\n",
    "v1 = model.get_embedding('I am going to a place')\n",
    "v2 = model.get_embedding('I am not going')\n",
    "print v1.shape, v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4187516]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.pairwise.cosine_distances(v1, v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
